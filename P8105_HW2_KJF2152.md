P8105_HW2_KJF2152
================
Kaleb J. Frierson
2024-09-30

- [Calling Libraries](#calling-libraries)
- [Problem 1](#problem-1)
  - [Read & Clean](#read--clean)
  - [Tidy Data?](#tidy-data)
  - [Investigate](#investigate)
- [Problem 2](#problem-2)
  - [Read & Clean](#read--clean-1)
  - [Writing](#writing)
- [Problem 3](#problem-3)
  - [Data Import & Setup](#data-import--setup)
  - [Description](#description)
  - [Table](#table)
  - [I, C, T, O Viewership](#i-c-t-o-viewership)

# Calling Libraries

``` r
library(tidyverse)
library(haven)
library(readxl)
library(knitr)
```

# Problem 1

## Read & Clean

Read and clean the data; retain line, station, name, station latitude /
longitude, routes served, entry, vending, entrance type, and ADA
compliance. Convert the entry variable from character (YES vs NO) to a
logical variable (the ifelse or case_match function may be useful).

``` r
mta = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           show_col_types = FALSE) |> 
  janitor::clean_names()


 mta_clean = 
   mta |> 
  select(line, station_name, station_latitude, station_longitude, route1:route11 ,
         entry, vending, entrance_type, ada) |> 
    mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |> 
    mutate(across(route1:route11, as.character)) 
```

## Tidy Data?

Write a short paragraph about this dataset – explain briefly what
variables the dataset contains, describe your data cleaning steps so
far, and give the dimension (rows x columns) of the resulting dataset.
Are these data tidy?

``` r
nrow(mta_clean)
```

    ## [1] 1868

``` r
ncol(mta_clean)
```

    ## [1] 19

``` r
head(mta_clean)
```

    ## # A tibble: 6 × 19
    ##   line     station_name station_latitude station_longitude route1 route2 route3
    ##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ## 1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <chr>, route9 <chr>, route10 <chr>, route11 <chr>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

``` r
summary(mta_clean)
```

    ##      line           station_name       station_latitude station_longitude
    ##  Length:1868        Length:1868        Min.   :40.58    Min.   :-74.03   
    ##  Class :character   Class :character   1st Qu.:40.69    1st Qu.:-73.99   
    ##  Mode  :character   Mode  :character   Median :40.73    Median :-73.96   
    ##                                        Mean   :40.73    Mean   :-73.94   
    ##                                        3rd Qu.:40.77    3rd Qu.:-73.91   
    ##                                        Max.   :40.90    Max.   :-73.76   
    ##     route1             route2             route3             route4         
    ##  Length:1868        Length:1868        Length:1868        Length:1868       
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     route5             route6             route7             route8         
    ##  Length:1868        Length:1868        Length:1868        Length:1868       
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     route9            route10            route11            entry        
    ##  Length:1868        Length:1868        Length:1868        Mode :logical  
    ##  Class :character   Class :character   Class :character   FALSE:115      
    ##  Mode  :character   Mode  :character   Mode  :character   TRUE :1753     
    ##                                                                          
    ##                                                                          
    ##                                                                          
    ##    vending          entrance_type         ada         
    ##  Length:1868        Length:1868        Mode :logical  
    ##  Class :character   Class :character   FALSE:1400     
    ##  Mode  :character   Mode  :character   TRUE :468      
    ##                                                       
    ##                                                       
    ## 

**The initial dataset (which I nicknamed “mta” in my original dataframe)
had 1868 rows and 32 columns. I created a new dataframe called
“mta_clean” which used mta, kept the assigned variables, and turned the
“entry” variable into a logical variable. In this new dataset there are
1868 rows and 19 columns. The way that route is recorded is not tidy. I
think it would make more sense to have a column for route that assigned
one route name per row (so each entry would probably have mutiple rows).
I was going to make it longer now; however, doing so would make it more
difficult to answer a question about proportion of entrances with
vending below.**

## Investigate

How many distinct stations are there? Note that stations are identified
both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway;
125st Lenox); the distinct function may be useful here.

**There are 465 distinct stations.**

``` r
num_distinct_stations =
  mta_clean |> 
  distinct(line, station_name) 

nrow(num_distinct_stations)
```

    ## [1] 465

How many stations are ADA compliant?

**84 stations are ADA compliant.**

``` r
num_ada_true =
  mta_clean |> 
  filter(ada == TRUE) |> 
  distinct(line, station_name) 

nrow(num_ada_true)
```

    ## [1] 84

What proportion of entrances without vending allow entrance?

**37.7% of the station entrances without vending allow entrance.**

``` r
prop =
  mta_clean |> 
  filter(vending == "NO") |> 
  summarise(proportion = sum(entry == TRUE) / n()) |> 
  pull(proportion)

prop
```

    ## [1] 0.3770492

Reformat data so that route number and route name are distinct
variables. How many distinct stations serve the A train?

**60 distinct stations serve the A train.**

``` r
mta_longer = 
  mta_clean |> 
  pivot_longer(
    cols = route1:route11, 
    names_to = "route_var", 
    values_to = "route_name"
  ) |> 
  filter(!is.na(route_name)) |>   
  separate(
    route_var, 
    into = c("route_number", "route_prefix"), 
    sep = "route", 
    convert = TRUE  
  ) |> 
  select(-route_prefix) 

a_train =
  mta_longer |> 
  filter(route_name == "A") |> 
  distinct(line, station_name, ada) 

nrow(a_train)
```

    ## [1] 60

Of the stations that serve the A train, how many are ADA compliant?

**17 of the stations that serve the A train are ADA compliant.**

``` r
compliant = 
  a_train |> 
  filter(ada == TRUE) 

nrow(compliant)
```

    ## [1] 17

``` bash
git config --global http.postBuffer 157286400
git config --global --get http.postBuffer # troubleshooting github push error
```

    ## 157286400

# Problem 2

## Read & Clean

Read and clean the Mr. Trash Wheel sheet

Specify the sheet in the Excel file and to omit non-data entries (rows
with notes / figures; columns containing notes) using arguments in
read_excel. Use reasonable variable names. Omit rows that do not include
dumpster-specific data. Round the number of sports balls to the nearest
integer and convert the result to an integer variable (using
as.integer).

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to both
datasets before combining.

``` r
mr_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 1) |> 
  janitor::clean_names() |>
    select(-c(x15, x16)) |> 
      na.omit() |> 
        mutate(sports_balls = as.integer(round(sports_balls))) |> 
  mutate(trash_wheel = "Mr. Trash Wheel") |> 
  mutate(year = as.numeric(year))
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
head(mr_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ## 6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

``` r
prof_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 2) |> 
    janitor::clean_names() |>
  na.omit() |> 
  mutate(trash_wheel = "Prof. Trash Wheel")

head(mr_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ## 6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

``` r
gwyn_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 4) |> 
    janitor::clean_names() |>
  na.omit() |> 
  mutate(trash_wheel = "Gwynnda")

head(mr_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ## 6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

``` r
colnames(mr_df)
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "sports_balls"       "homes_powered"      "trash_wheel"

``` r
colnames(prof_df)
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "glass_bottles"      "plastic_bags"       "wrappers"          
    ## [13] "homes_powered"      "trash_wheel"

``` r
colnames(gwyn_df)
```

    ##  [1] "dumpster"           "month"              "year"              
    ##  [4] "date"               "weight_tons"        "volume_cubic_yards"
    ##  [7] "plastic_bottles"    "polystyrene"        "cigarette_butts"   
    ## [10] "plastic_bags"       "wrappers"           "homes_powered"     
    ## [13] "trash_wheel"

``` r
merged_df = bind_rows(mr_df, prof_df, gwyn_df)

nrow(mr_df)
```

    ## [1] 629

``` r
nrow(prof_df)
```

    ## [1] 113

``` r
nrow(gwyn_df)
```

    ## [1] 103

``` r
num = nrow(mr_df)+nrow(prof_df)+nrow(gwyn_df)
num
```

    ## [1] 845

``` r
nrow(merged_df)
```

    ## [1] 845

## Writing

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in June of 2022?

**The `mr_df` dataset had `nrow(mr_df)`observations, Professor Trash
Wheel `nrow(prof_df)` and Gwynnda `nrow(gwynn)`. There are
`nrow(merged_df)` observations in the joined dataset `merged_df` which
is equivalent to the sum of the observations in the three datasets. See
`num` above. There are 15 columns including date of dumpster recording,
the dumpster number for each specific trash wheel, tons of trash
collected per collection time, and weights of various different types of
waste. For example, Professor Trash Wheel collected `prof_trash_weight`
tons of trash and Gwynnda collected `cigs_june_gwyn` cigarette butts in
June of 2022.**

``` r
prof_trash_weight=
  merged_df |> 
  filter(trash_wheel == "Prof. Trash Wheel")  |> 
  summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))

cigs_june_gwyn=
  merged_df |> 
  filter(trash_wheel == "Gwynnda", month == "June", year == 2022) |> 
  nrow()
```

# Problem 3

## Data Import & Setup

This problem uses data on elements of the Great British Bake Off. The
show has been running for 10 seasons; in each episode, contestants
compete in signature challenges, technical challenges, and a
showstopper. At the end of an episode the winner is crowned “Star Baker”
(and winner in the last episode of a season), and a loser is eliminated.

Information about individual bakers, their bakes, and their performance
is included in bakers.csv, bakes.csv, and results.csv. In the first part
of this problem, your goal is to create a single, well-organized dataset
with all the information contained in these data files. To that end:
import, clean, tidy, and otherwise wrangle each of these datasets; check
for completeness and correctness across datasets (e.g. by viewing
individual datasets and using anti_join); merge to create a single,
final dataset; and organize this so that variables and observations are
in meaningful orders. Export the result as a CSV in the directory
containing the original datasets.

Here I import bakers csv and assigned it clean names using janitor. It
has `nrow(bakers)` observations.

``` r
bakers = 
  read_csv("Data/gbb_datasets/bakers.csv", show_col_types = FALSE) |> 
  janitor::clean_names()

nrow(bakers)
```

    ## [1] 120

Here I import bakes csv and assigned it clean names using janitor. It
has `nrow(bakes)` observations.

``` r
bakes = 
  read_csv("Data/gbb_datasets/bakes.csv", show_col_types = FALSE) |> 
  janitor::clean_names()

nrow(bakes)
```

    ## [1] 548

Here I import results csv and assigned it clean names using janitor. I
also skip the first two rows which have no data and omit any rows
missing data. I opted to do this because without outcome information
this dataset has no utility. It has `nrow(results)` observations.

``` r
results = 
  read_csv("Data/gbb_datasets/results.csv", show_col_types = FALSE,
           skip = 2) |> 
  na.omit()
  
nrow(results) 
```

    ## [1] 696

Here I joined the bakes df into the results df I did it in such a way
because the results df had the most observations. Bakes didn’t have
complete data after series 8.

``` r
results_bakes = 
  results |> 
  left_join(bakes, by =c("series", "episode", "baker"))
```

Here I separated the baker_name into two variables because it was the
only df that included last name. I held onto the original variable in
case something went wrong. You will see I remove it later.

``` r
bakers_names=
  bakers |> 
  separate(baker_name, into = c("baker", "last_name"), sep = " ", extra = "merge", remove = FALSE)
```

Here is where I “remove” it by selecting which variables I want in the
final dataset after merging bakers (now called `bakers_names`) into
`results_bakes`.

``` r
results_bakes_names=
  results_bakes |> 
  left_join(bakers_names, by = c("baker", "series")) |> 
  select(baker, last_name, baker_age, baker_occupation, hometown, series, episode, result, technical, signature_bake, show_stopper)

results_bakes_names
```

    ## # A tibble: 696 × 11
    ##    baker     last_name baker_age baker_occupation hometown series episode result
    ##    <chr>     <chr>         <dbl> <chr>            <chr>     <dbl>   <dbl> <chr> 
    ##  1 Annetha   Mills            30 Midwife          Essex         1       1 IN    
    ##  2 David     Chambers         31 Entrepreneur     Milton …      1       1 IN    
    ##  3 Edd       Kimber           24 Debt collector … Bradford      1       1 IN    
    ##  4 Jonathan  Shepherd         25 Research Analyst St Alba…      1       1 IN    
    ##  5 Miranda   Browne           37 Food buyer for … Midhurs…      1       1 IN    
    ##  6 Lea       Harris           51 Retired          Midloth…      1       1 OUT   
    ##  7 David     Chambers         31 Entrepreneur     Milton …      1       2 IN    
    ##  8 Edd       Kimber           24 Debt collector … Bradford      1       2 IN    
    ##  9 Jasminder Randhawa         45 Assistant Credi… Birming…      1       2 IN    
    ## 10 Jonathan  Shepherd         25 Research Analyst St Alba…      1       2 IN    
    ## # ℹ 686 more rows
    ## # ℹ 3 more variables: technical <dbl>, signature_bake <chr>, show_stopper <chr>

Here I export the merged final analytic df as a csv to this project’s
data-folder.

``` r
write_csv(results_bakes_names, "Data/gbb_datasets/20241001_results_bakes_names.csv")
```

## Description

Describe your data cleaning process, including any questions you have or
choices you made. Briefly discuss the final dataset.

**Throughout the above steps I load, clean, and merge the three separate
datasets (bakers, bakes, results) by their common columns. I omitted
rows in the results dataset and created a new name variable that
separated first and last name. This new dataset has
`nrow(results_bakes_names)` observations and 11 columns. Each of the
columns I chose to keep and in what order as assigned above.**

## Table

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises?

**I am not surprised. Honestly I have no emotions related to the winner
(or loser) of this show. I think the table looks okay, maybe not
publishable. I would like to learn to play with the spacing and titles,
etc. **

``` r
star_baker_table = 
  results_bakes_names |> 
  filter(series >= 5, result %in% c("STAR BAKER", "WINNER")) |> 
  select(series, episode, baker, result) |> 
  arrange(series, episode)

star_baker_table |> 
  kable(caption = "Star Bakers and Winners from Series 5 and Up")
```

| series | episode | baker     | result     |
|-------:|--------:|:----------|:-----------|
|      5 |       1 | Nancy     | STAR BAKER |
|      5 |       2 | Richard   | STAR BAKER |
|      5 |       3 | Luis      | STAR BAKER |
|      5 |       4 | Richard   | STAR BAKER |
|      5 |       5 | Kate      | STAR BAKER |
|      5 |       6 | Chetna    | STAR BAKER |
|      5 |       7 | Richard   | STAR BAKER |
|      5 |       8 | Richard   | STAR BAKER |
|      5 |       9 | Richard   | STAR BAKER |
|      5 |      10 | Nancy     | WINNER     |
|      6 |       1 | Marie     | STAR BAKER |
|      6 |       2 | Ian       | STAR BAKER |
|      6 |       3 | Ian       | STAR BAKER |
|      6 |       4 | Ian       | STAR BAKER |
|      6 |       5 | Nadiya    | STAR BAKER |
|      6 |       6 | Mat       | STAR BAKER |
|      6 |       7 | Tamal     | STAR BAKER |
|      6 |       8 | Nadiya    | STAR BAKER |
|      6 |       9 | Nadiya    | STAR BAKER |
|      6 |      10 | Nadiya    | WINNER     |
|      7 |       1 | Jane      | STAR BAKER |
|      7 |       2 | Candice   | STAR BAKER |
|      7 |       3 | Tom       | STAR BAKER |
|      7 |       4 | Benjamina | STAR BAKER |
|      7 |       5 | Candice   | STAR BAKER |
|      7 |       6 | Tom       | STAR BAKER |
|      7 |       7 | Andrew    | STAR BAKER |
|      7 |       8 | Candice   | STAR BAKER |
|      7 |       9 | Andrew    | STAR BAKER |
|      7 |      10 | Candice   | WINNER     |
|      8 |       1 | Steven    | STAR BAKER |
|      8 |       2 | Steven    | STAR BAKER |
|      8 |       3 | Julia     | STAR BAKER |
|      8 |       4 | Kate      | STAR BAKER |
|      8 |       5 | Sophie    | STAR BAKER |
|      8 |       6 | Liam      | STAR BAKER |
|      8 |       7 | Steven    | STAR BAKER |
|      8 |       8 | Stacey    | STAR BAKER |
|      8 |       9 | Sophie    | STAR BAKER |
|      8 |      10 | Sophie    | WINNER     |
|      9 |       1 | Manon     | STAR BAKER |
|      9 |       2 | Rahul     | STAR BAKER |
|      9 |       3 | Rahul     | STAR BAKER |
|      9 |       4 | Dan       | STAR BAKER |
|      9 |       5 | Kim-Joy   | STAR BAKER |
|      9 |       6 | Briony    | STAR BAKER |
|      9 |       7 | Kim-Joy   | STAR BAKER |
|      9 |       8 | Ruby      | STAR BAKER |
|      9 |       9 | Ruby      | STAR BAKER |
|      9 |      10 | Rahul     | WINNER     |
|     10 |       1 | Michelle  | STAR BAKER |
|     10 |       2 | Alice     | STAR BAKER |
|     10 |       3 | Michael   | STAR BAKER |
|     10 |       4 | Steph     | STAR BAKER |
|     10 |       5 | Steph     | STAR BAKER |
|     10 |       6 | Steph     | STAR BAKER |
|     10 |       7 | Henry     | STAR BAKER |
|     10 |       8 | Steph     | STAR BAKER |
|     10 |       9 | Alice     | STAR BAKER |
|     10 |      10 | David     | WINNER     |

Star Bakers and Winners from Series 5 and Up

## I, C, T, O Viewership

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset. What was the average viewership
in Season 1? In Season 5?

**Average viewership in series 1 was `viewers_s1`, average viewership in
series 5 was `viewers_s5`.**

``` r
viewers=
  read_csv("Data/gbb_datasets/viewers.csv", show_col_types = FALSE) |>
  janitor::clean_names() |> 
  pivot_longer(
    cols = series_1:series_10, 
    names_to = "series", 
    values_to = "viewership"
  ) |> 
  mutate(series = str_remove(series, "series_")) |> 
  select(series, episode, viewership)

head(viewers, 10)
```

    ## # A tibble: 10 × 3
    ##    series episode viewership
    ##    <chr>    <dbl>      <dbl>
    ##  1 1            1       2.24
    ##  2 2            1       3.1 
    ##  3 3            1       3.85
    ##  4 4            1       6.6 
    ##  5 5            1       8.51
    ##  6 6            1      11.6 
    ##  7 7            1      13.6 
    ##  8 8            1       9.46
    ##  9 9            1       9.55
    ## 10 10           1       9.62

``` r
viewers_s1 = 
  viewers |> 
  filter(series == "1") |> 
  na.omit() |> 
  summarise(avg_viewership = mean(viewership))

viewers_s5 = 
  viewers |> 
  filter(series == "5") |> 
  na.omit() |> 
  summarise(avg_viewership = mean(viewership))

viewers_s1
```

    ## # A tibble: 1 × 1
    ##   avg_viewership
    ##            <dbl>
    ## 1           2.77

``` r
viewers_s5
```

    ## # A tibble: 1 × 1
    ##   avg_viewership
    ##            <dbl>
    ## 1           10.0
