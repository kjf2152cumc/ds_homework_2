---
title: "P8105_HW2_KJF2152"
author: "Kaleb J. Frierson"
date: "2024-09-30"
output: 
  github_document:
    toc: TRUE
---

# Calling Libraries

```{r load packages, echo= TRUE, message= FALSE}
library(tidyverse)
library(haven)
library(readxl)
```

# Problem 1

## Read & Clean

Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or case_match function may be useful).

```{r read/clean}
mta = 
  read_csv("Data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           show_col_types = FALSE) |> 
  janitor::clean_names()


 mta_clean = 
   mta |> 
  select(line, station_name, station_latitude, station_longitude, route1:route11 ,
         entry, vending, entrance_type, ada) |> 
    mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |> 
    mutate(across(route1:route11, as.character)) 

```

## Tidy Data? 

Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

```{r summary stats}
nrow(mta_clean)
ncol(mta_clean)
head(mta_clean)
summary(mta_clean)
```

The initial dataset (which I nicknamed "mta" in my original dataframe) had 1868 rows and 32 columns. I created a new dataframe called "mta_clean" which used mta, kept the assigned variables, and turned the "entry" variable into a logical variable. In this new dataset there are 1868 rows and 19 columns. The way that route is recorded is not tidy. I think it would make more sense to have a column for route that assigned one route name per row (so each entry would probably have mutiple rows). I was going to make it longer now; however, doing so would make it more difficult to answer a question about proportion of entrances with vending below. 

## Investigate

How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

**There are 465 distinct stations.**

```{r stations}
num_distinct_stations =
  mta_clean |> 
  distinct(line, station_name) 

nrow(num_distinct_stations)
```

How many stations are ADA compliant?

**84 stations are ADA compliant.**

```{r ada stations}
num_ada_true =
  mta_clean |> 
  filter(ada == TRUE) |> 
  distinct(line, station_name) 

nrow(num_ada_true)
```

What proportion of entrances without vending allow entrance?

**37.7% of the station entrances without vending allow entrance.**

```{r proportion of ent. }
prop =
  mta_clean |> 
  filter(vending == "NO") |> 
  summarise(proportion = sum(entry == TRUE) / n()) |> 
  pull(proportion)

prop
```

Reformat data so that route number and route name are distinct variables. How many distinct stations serve the A train? 

**60 distinct stations serve the A train.**

```{r pivot}
mta_longer = 
  mta_clean |> 
  pivot_longer(
    cols = route1:route11, 
    names_to = "route_var", 
    values_to = "route_name"
  ) |> 
  filter(!is.na(route_name)) |>   
  separate(
    route_var, 
    into = c("route_number", "route_prefix"), 
    sep = "route", 
    convert = TRUE  
  ) |> 
  select(-route_prefix) 

a_train =
  mta_longer |> 
  filter(route_name == "A") |> 
  distinct(line, station_name, ada) 

nrow(a_train)

```

Of the stations that serve the A train, how many are ADA compliant?

**17 of the stations that serve the A train are ADA compliant.**

```{r a train ada}
compliant = 
  a_train |> 
  filter(ada == TRUE) 

nrow(compliant)
```
```{bash}
git config --global http.postBuffer 157286400
git config --global --get http.postBuffer # troubleshooting github push error
```


# Problem 2

## Read & Clean

Read and clean the Mr. Trash Wheel sheet

Specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel. Use reasonable variable names. Omit rows that do not include dumpster-specific data. Round the number of sports balls to the nearest integer and convert the result to an integer variable (using as.integer). 

Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.

```{r mr_trash_wheel}
mr_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 1) |> 
  janitor::clean_names() |>
    select(-c(x15, x16)) |> 
      na.omit() |> 
        mutate(sports_balls = as.integer(round(sports_balls))) |> 
  mutate(trash_wheel = "Mr. Trash Wheel") |> 
  mutate(year = as.numeric(year))

head(mr_df)
```

```{r prof_trash_wheel}
prof_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 2) |> 
    janitor::clean_names() |>
  na.omit() |> 
  mutate(trash_wheel = "Prof. Trash Wheel")

head(mr_df)
```

```{r gwynnda_trash_wheel}
gwyn_df = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = 4) |> 
    janitor::clean_names() |>
  na.omit() |> 
  mutate(trash_wheel = "Gwynnda")

head(mr_df)
```

```{r check columns}
colnames(mr_df)
colnames(prof_df)
colnames(gwyn_df)
```

```{r join}
merged_df = bind_rows(mr_df, prof_df, gwyn_df)

nrow(mr_df)
nrow(prof_df)
nrow(gwyn_df)

num = nrow(mr_df)+nrow(prof_df)+nrow(gwyn_df)
num

nrow(merged_df)
```

## Writing

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in June of 2022?

The `mr_df` dataset had `nrow(mr_df)`observations, Professor Trash Wheel `nrow(prof_df)` and Gwynnda `nrow(gwynn)`. There are `nrow(merged_df)` observations in the joined dataset `merged_df` which is equivalent to the sum of the observations in the three datasets. See `num` above. There are 15 columns including date of dumpster recording, the dumpster number for each specific trash wheel, tons of trash collected per collection time, and weights of various different types of waste. For example, Professor Trash Wheel collected `prof_trash_weight` tons of trash and Gwynnda collected `cigs_june_gwyn` cigarette butts in June of 2022.  

```{r writing}
prof_trash_weight=
  merged_df |> 
  filter(trash_wheel == "Prof. Trash Wheel")  |> 
  summarise(total_weight_tons = sum(weight_tons, na.rm = TRUE))

cigs_june_gwyn=
  merged_df |> 
  filter(trash_wheel == "Gwynnda", month == "June", year == 2022) |> 
  nrow()
```


# Problem 3

## Data Import & Setup

This problem uses data on elements of the Great British Bake Off. The show has been running for 10 seasons; in each episode, contestants compete in signature challenges, technical challenges, and a showstopper. At the end of an episode the winner is crowned “Star Baker” (and winner in the last episode of a season), and a loser is eliminated.

Information about individual bakers, their bakes, and their performance is included in bakers.csv, bakes.csv, and results.csv. In the first part of this problem, your goal is to create a single, well-organized dataset with all the information contained in these data files. To that end: import, clean, tidy, and otherwise wrangle each of these datasets; check for completeness and correctness across datasets (e.g. by viewing individual datasets and using anti_join); merge to create a single, final dataset; and organize this so that variables and observations are in meaningful orders. Export the result as a CSV in the directory containing the original datasets.

Here I import bakers csv and assigned it clean names using janitor. It has `nrow(bakers)` observations. 
```{r bakers}
bakers = 
  read_csv("Data/gbb_datasets/bakers.csv", show_col_types = FALSE) |> 
  janitor::clean_names()

nrow(bakers)
```
Here I import bakes csv and assigned it clean names using janitor. It has `nrow(bakes)` observations. 
```{r bakes}
bakes = 
  read_csv("Data/gbb_datasets/bakes.csv", show_col_types = FALSE) |> 
  janitor::clean_names()

nrow(bakes)
```
Here I import results csv and assigned it clean names using janitor. I also skip the first two rows which have no data and omit any rows missing data. I opted to do this because without outcome information this dataset has no utility. It has `nrow(results)` observations. 
```{r reslts}
results = 
  read_csv("Data/gbb_datasets/results.csv", show_col_types = FALSE,
           skip = 2) |> 
  na.omit()
  
nrow(results) 
```
Here I joined the bakes df into the results df I did it in such a way because the results df had the most observations. Bakes didn't have complete data after season 8. 
```{r first join}
results_bakes = 
  results |> 
  left_join(bakes, by =c("series", "episode", "baker"))
```

Here I separated the baker_name into two variables because it was the only df that included last name. I held onto the original variable in case something went wrong. You will see I remove it later. 
```{r last names}
bakers_names=
  bakers |> 
  separate(baker_name, into = c("baker", "last_name"), sep = " ", extra = "merge", remove = FALSE)

```

Here is where I "remove" it by selecting which variables I want in the final dataset after merging bakers (now called `bakers_names`) into `results_bakes`. 
```{r}
results_bakes_names=
  results_bakes |> 
  left_join(bakers_names, by = c("baker", "series")) |> 
  select(baker, last_name, baker_age, baker_occupation, hometown, series, episode, result, technical, signature_bake, show_stopper)

results_bakes_names
```

Here I export the merged final analytic df as a csv to this project's data-folder. 
```{r}
write_csv(results_bakes_names, "Data/gbb_datasets/20241001_results_bakes_names.csv")
```


## Description 

Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

Throughout the above steps clean and merge the three separate datasets (bakers, bakes, results) by their common columns (season and baker).
During this process, I ensured that there were no duplicated rows and checked for consistent column names.
New variables were added (e.g., star_baker and result) to indicate performance in each season.

## Table 
Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?

Create a table showing the star baker or winner for each episode in Seasons 5 through 10
star_baker_table <- final_df |> 
  filter(season >= 5, !is.na(star_baker)) |> 
  select(season, episode, baker, result) |> 
  arrange(season, episode)

Display the table
print(star_baker_table)

## I, C, T, O Viewership
Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. What was the average viewership in Season 1? In Season 5?

Import, clean, tidy, and organize the viewership data in viewers.csv
viewers <- read_csv("Data/gbb_datasets/viewers.csv", show_col_types = FALSE)

Clean and organize viewership data
viewers_clean <- viewers |> 
  clean_names() |> 
  group_by(season) |> 
  summarise(average_viewership = mean(viewers, na.rm = TRUE))

Show the first 10 rows of the cleaned viewership dataset
head(viewers_clean, 10)

What was the average viewership in Season 1? In Season 5?
season_1_viewership <- viewers_clean |> filter(season == 1) |> pull(average_viewership)
season_5_viewership <- viewers_clean |> filter(season == 5) |> pull(average_viewership)

Output the results
season_1_viewership
season_5_viewership

